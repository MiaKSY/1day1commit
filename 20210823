=====================
스파크 설치
=====================



<dn01의 root 계정에서 >

su - root
vagrant


[root@dn01 ~]# cd /tmp

[root@dn01 tmp]# wget https://archive.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz

* 에러 발생 : wget http://apache.mirror.cdnetworks.com/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz

[root@dn01 tmp]# tar xzvf spark-2.4.7-bin-hadoop2.7.tgz

[root@dn01 tmp]# mkdir -p /opt/spark/2.4.7

[root@dn01 tmp]# mv spark-2.4.7-bin-hadoop2.7/* /opt/spark/2.4.7/

[root@dn01 tmp]# ln -s /opt/spark/2.4.7 /opt/spark/current

[root@dn01 tmp]# chown -R hadoop:hadoop /opt/spark/

[root@dn01 tmp]# su - hadoop


< hadoop 계정 >

[hadoop@dn01 ~]$ vi ~/.bash_profile



###### spark ######################
export SPARK_HOME=/opt/spark/current
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin
#### spark ######################


환경변수 적용
[hadoop@dn01 ~]$ source ~/.bash_profile



[hadoop@dn01 conf]$ cd $SPARK_HOME/conf


ls
하면 파일들이 나온다.

-----------------------
mv slaves.template slaves
vi slaves
-----------------------

nn01
dn02


( 기존 localhost는 지우자 )



-----------------------
mv spark-defaults.conf.template spark-defaults.conf
vi spark-defaults.conf
-----------------------

spark.yarn.jars /opt/spark/current/jars/*





-----------------------
mv log4j.properties.template log4j.properties
vi log4j.properties
-----------------------


#INFO -> ERROR로 바꿔 줌 -> Spark 로그에 정신없는 INFO가 안나타남

log4j.rootCategory=ERROR, console






-----------------------
mv spark-env.sh.template spark-env.sh
vi spark-env.sh
-----------------------

SPARK_MASTER_HOST=dn01
export JAVA_HOME=/opt/jdk/current
export HADOOP_HOME=/opt/hadoop/current
export SPARK_HOME=/opt/spark/current
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_DRIVER_MEMORY=2g
export SPARK_EXECUTOR_INSTANCES=2
export SPARK_EXECUTOR_CORES=1
export SPARK_EXECUTOR_MEMORY=2g
export SPARK_MASTER_IP=192.168.56.102
#export SPARK_WORKER_DIR=/spark_data/spwork
#export SPARK_PID_DIR=/spark_data/sptmp
export SPARK_DIST_CLASSPATH=$(/opt/hadoop/current/bin/hadoop classpath):/opt/spark/current/jars/*
#export PYTHONPATH=/opt/python/current/python3
#export PYSPARK_PYTHON=/opt/python/current/python3



스파크에 접속하는 쉘 열기
[hadoop@dn01 ] spark-shell

spark-shell 확인

(Welcome to SPARK~)
( 스파크 언어를 써야 한다. )

scala> sc.setLogLevel("WARN")

scala> val f = sc.textFile("file:///etc/hosts")

scala> f.count

scala> f.first

scala> f.collect

scala> :quit


스파크에 접속하는 쉘 열기
[hadoop@dn01 ]$ spark-shell

scala> val x = sc.parallelize(List("hello","hi","java"))
scala> val y = x.map(x=>(x,1))
scala> y.collect
scala> :quit

파이썬스파크에 접속하는 쉘 열기
[hadoop@dn01 ]$ pyspark

>>> x = sc.parallelize(["hello","hi","python"])
>>> y = x.map
>>>
>>>
>>>
>>> exit()






=================================================





[ 방법 1 ]

[ dn01 노드에서 dn02와 nn01로 전송 복사 ]

[hadoop@dn02 ~]$ vi ~/.bash_profile

###### spark ######################
export SPARK_HOME=/opt/spark/current
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin
#### spark ######################

[hadoop@dn02 ~]$ source ~/.bash_profile



[hadoop@nn01 ~]$ vi ~/.bash_profile

###### spark ######################
export SPARK_HOME=/opt/spark/current
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin
#### spark ######################

[hadoop@nn01 ~]$ source ~/.bash_profile


---------------------------------------------------

[ 방법 2 ]

[ dn01에서 scp로 전송 ]

sudo 명령어를 사용하던가 su - root 계정 변경하여 복사

$ sudo scp -r /opt/spark dn02:/opt/spark
$ sudo scp -r /opt/spark nn01:/opt/spark

scp로 전송하면 softlink는 안잡힌다.
current가 디렉토리로 된 거 확인하고 디렉토디를 지우고 softlink로 설정
dn02와 nn01 [ root 계정 ] 에서 확인 

[root@dn02 ~]# rm -rf /opt/spark/current
[root@dn02 ~]# ln -s /opt/spark/2.4.7 /opt/spark/current
[root@dn02 ~]# ll /opt/spark/
[root@dn02 ~]# chown -R hadoop:hadoop /opt/spark/
[root@dn02 ~]# su - hadoop

---------------------------------------------------



